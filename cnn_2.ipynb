{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Electricity price prediction for NYC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from sklearn import metrics\n",
    "import matplotlib.pylab as plt\n",
    "import datetime as dt\n",
    "import time\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.recurrent import LSTM, GRU\n",
    "from keras.layers import Convolution1D, MaxPooling1D, AtrousConvolution1D, RepeatVector\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, CSVLogger\n",
    "from keras.layers.wrappers import Bidirectional\n",
    "from keras import regularizers\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.advanced_activations import *\n",
    "from keras.optimizers import RMSprop, Adam, SGD, Nadam\n",
    "from keras.initializers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data = pd.read_csv('full_diff_data_2016_2017.csv', parse_dates=['time_stamp', 'time_stamp_local'])\n",
    "df_data.set_index('time_stamp', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17520, 10)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['time_stamp_local', 'price', 'price_24hr_ago', 'price_diff', 'load',\n",
       "       'load_24hr_ago', 'load_diff', 'temp', 'temp_24hr_ago', 'temp_diff'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_ts_split(X, y, percentage=0.9):\n",
    "    X_train = X[0:int(len(X) * percentage)]\n",
    "    Y_train = y[0:int(len(y) * percentage)]\n",
    "    \n",
    "    #X_train, Y_train = shuffle_Xy(X_train, Y_train)\n",
    "\n",
    "    X_test = X[int(len(X) * percentage):]\n",
    "    Y_test = y[int(len(X) * percentage):]\n",
    "\n",
    "    return X_train, X_test, Y_train, Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_price = preprocessing.StandardScaler().fit(df_data.price.values.reshape(-1, 1))\n",
    "scaler_load = preprocessing.StandardScaler().fit(df_data.load.values.reshape(-1, 1))\n",
    "scaler_temp = preprocessing.StandardScaler().fit(df_data.temp.values.reshape(-1, 1))\n",
    "# # scaler_price = preprocessing.MinMaxScaler().fit(df_data.price_diff.values.reshape(-1, 1))\n",
    "# # scaler_load = preprocessing.MinMaxScaler().fit(df_data.load_diff.values.reshape(-1, 1))\n",
    "# # scaler_temp = preprocessing.MinMaxScaler().fit(df_data.temp_diff.values.reshape(-1, 1))\n",
    "\n",
    "df_data.loc[:,'price_norm'] = scaler_price.transform(df_data.price.values.reshape(-1, 1))\n",
    "df_data.loc[:,'load_norm'] = scaler_load.transform(df_data.load.values.reshape(-1, 1))\n",
    "df_data.loc[:,'temp_norm'] = scaler_temp.transform(df_data.temp.values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "WINDOW = 24 * 7 - 1\n",
    "EMB_SIZE = 3\n",
    "STEP = 1\n",
    "FORECAST = 1\n",
    "\n",
    "X, Y = [], []\n",
    "for i in range(0, df_data.shape[0], STEP): \n",
    "    try:\n",
    "        price_byweek = df_data.price_norm.values[i: i + WINDOW]\n",
    "        load_byweek = df_data.load_norm.values[i: i + WINDOW]\n",
    "        temp_byweek = df_data.temp_norm.values[i: i + WINDOW]\n",
    "\n",
    "#         price_byweek = df_data.price.values[i: i + WINDOW]\n",
    "#         load_byweek = df_data.load.values[i: i + WINDOW]\n",
    "#         temp_byweek = df_data.temp.values[i: i + WINDOW]\n",
    "\n",
    "        \n",
    "#         price_norm = (np.array(price_byweek) - np.mean(price_byweek)) / np.std(price_byweek)\n",
    "#         load_norm = (np.array(load_byweek) - np.mean(load_byweek)) / np.std(load_byweek)\n",
    "#         temp_norm = (np.array(temp_byweek) - np.mean(temp_byweek)) / np.std(temp_byweek)\n",
    "        \n",
    "#         x_i = np.column_stack((price_norm, load_norm, temp_norm))\n",
    "        x_i = np.column_stack((price_byweek, load_byweek, temp_byweek))\n",
    "\n",
    "        y_i = df_data.price.values[i + WINDOW + FORECAST]\n",
    "    except Exception as e:\n",
    "        break\n",
    "\n",
    "    X.append(x_i)\n",
    "    Y.append(y_i)\n",
    "\n",
    "X, Y = np.array(X), np.array(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((17352, 167, 3), (17352,))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((15616, 167, 3), (1736, 167, 3), (15616,), (1736,))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_ts_split(X, Y, percentage = 0.9)\n",
    "X_train.shape, X_test.shape, Y_train.shape, Y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((15616, 167, 3), (1736, 167, 3), (15616,), (1736,))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], EMB_SIZE))\n",
    "X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], EMB_SIZE))\n",
    "X_train.shape, X_test.shape, Y_train.shape, Y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py35/lib/python3.5/site-packages/ipykernel_launcher.py:5: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(kernel_size=24, input_shape=(167, 3), filters=32, padding=\"same\")`\n",
      "  \"\"\"\n",
      "/anaconda/envs/py35/lib/python3.5/site-packages/ipykernel_launcher.py:15: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(kernel_size=2, padding=\"same\", filters=16)`\n",
      "  from ipykernel import kernelapp as app\n",
      "/anaconda/envs/py35/lib/python3.5/site-packages/keras/models.py:874: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 12492 samples, validate on 3124 samples\n",
      "Epoch 1/30\n",
      "12492/12492 [==============================] - 11s 909us/step - loss: 813.2901 - val_loss: 473.8110\n",
      "Epoch 2/30\n",
      "12492/12492 [==============================] - 11s 854us/step - loss: 663.7734 - val_loss: 383.4331\n",
      "Epoch 3/30\n",
      "12492/12492 [==============================] - 11s 848us/step - loss: 630.6682 - val_loss: 326.8854\n",
      "Epoch 4/30\n",
      "12492/12492 [==============================] - 12s 964us/step - loss: 604.3726 - val_loss: 310.5981\n",
      "Epoch 5/30\n",
      "12492/12492 [==============================] - 11s 858us/step - loss: 586.7111 - val_loss: 297.2561\n",
      "Epoch 6/30\n",
      "12492/12492 [==============================] - 10s 830us/step - loss: 576.5124 - val_loss: 289.1455\n",
      "Epoch 7/30\n",
      "12492/12492 [==============================] - 11s 858us/step - loss: 558.4107 - val_loss: 287.3028\n",
      "Epoch 8/30\n",
      "12492/12492 [==============================] - 12s 921us/step - loss: 546.2055 - val_loss: 295.6822\n",
      "Epoch 9/30\n",
      "12492/12492 [==============================] - 11s 851us/step - loss: 525.2521 - val_loss: 332.8700\n",
      "Epoch 10/30\n",
      "12492/12492 [==============================] - 11s 847us/step - loss: 524.1729 - val_loss: 289.2049\n",
      "Epoch 11/30\n",
      "12492/12492 [==============================] - 11s 851us/step - loss: 510.2145 - val_loss: 298.0852\n",
      "Epoch 12/30\n",
      "12492/12492 [==============================] - 11s 850us/step - loss: 501.2118 - val_loss: 291.0024\n",
      "Epoch 13/30\n",
      "12492/12492 [==============================] - 11s 854us/step - loss: 484.9543 - val_loss: 321.9033\n",
      "Epoch 14/30\n",
      "12492/12492 [==============================] - 11s 850us/step - loss: 467.9599 - val_loss: 308.7008\n",
      "Epoch 15/30\n",
      "12492/12492 [==============================] - 11s 852us/step - loss: 470.0503 - val_loss: 300.3037\n",
      "Epoch 16/30\n",
      "12492/12492 [==============================] - 11s 854us/step - loss: 465.0318 - val_loss: 310.4373\n",
      "Epoch 17/30\n",
      "12492/12492 [==============================] - 11s 850us/step - loss: 434.5087 - val_loss: 344.9901\n",
      "Epoch 18/30\n",
      "12492/12492 [==============================] - 11s 863us/step - loss: 430.1259 - val_loss: 422.9882\n",
      "Epoch 19/30\n",
      "12492/12492 [==============================] - 11s 869us/step - loss: 425.2458 - val_loss: 337.4919\n",
      "Epoch 20/30\n",
      "12492/12492 [==============================] - 11s 854us/step - loss: 427.2014 - val_loss: 302.5823\n",
      "Epoch 21/30\n",
      "12492/12492 [==============================] - 11s 910us/step - loss: 400.6260 - val_loss: 307.4986\n",
      "Epoch 22/30\n",
      "12492/12492 [==============================] - 11s 882us/step - loss: 379.4141 - val_loss: 323.3240\n",
      "Epoch 23/30\n",
      "12492/12492 [==============================] - 11s 917us/step - loss: 374.7832 - val_loss: 344.6494\n",
      "Epoch 24/30\n",
      "12492/12492 [==============================] - 12s 932us/step - loss: 356.2783 - val_loss: 332.4895\n",
      "Epoch 25/30\n",
      "12492/12492 [==============================] - 12s 931us/step - loss: 347.7722 - val_loss: 340.3259\n",
      "Epoch 26/30\n",
      "12492/12492 [==============================] - 11s 852us/step - loss: 336.7778 - val_loss: 319.3482\n",
      "Epoch 27/30\n",
      " 8960/12492 [====================>.........] - ETA: 2s - loss: 364.1707"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Convolution1D(input_shape = (WINDOW, EMB_SIZE),\n",
    "                        nb_filter=32,\n",
    "                        filter_length=24,\n",
    "                        border_mode='same'))\n",
    "\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "# model.add(Dropout(0.5))\n",
    "\n",
    "\n",
    "model.add(MaxPooling1D(pool_size=2, strides=None, padding='valid'))\n",
    "model.add(Convolution1D(nb_filter=16,\n",
    "                        filter_length=2,\n",
    "                        border_mode='same'))\n",
    "\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "\n",
    "model.add(MaxPooling1D(pool_size=2, strides=None, padding='valid'))\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "# model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('linear'))\n",
    "\n",
    "model.compile(optimizer='adam', \n",
    "              loss='mse')\n",
    "history = model.fit(X_train, Y_train, \n",
    "          nb_epoch = 30, \n",
    "          batch_size = 128, \n",
    "          verbose=1, \n",
    "          validation_split=0.2,\n",
    "          shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model.h5')\n",
    "#model.load_weights(\"model.hdf5\")\n",
    "pred = model.predict(np.array(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='best')\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "# plt.plot(scaler_price.inverse_transform(Y_test))\n",
    "# plt.plot(scaler_price.inverse_transform(pred))\n",
    "plt.plot(Y_test)\n",
    "plt.plot(pred)\n",
    "\n",
    "\n",
    "plt.title('time series')\n",
    "# plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['Y_test', 'pred'], loc='best')\n",
    "plt.xlim(6000, 6100)\n",
    "plt.ylim(0,100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.sqrt(metrics.mean_squared_error(scaler_price.inverse_transform(Y_test), scaler_price.inverse_transform(pred)))\n",
    "np.sqrt(metrics.mean_squared_error(Y_test.ravel(), pred.ravel()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics.mean_absolute_error(scaler_price.inverse_transform(Y_test), scaler_price.inverse_transform(pred))\n",
    "np.sqrt(metrics.mean_absolute_error(Y_test.ravel(), pred.ravel()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
